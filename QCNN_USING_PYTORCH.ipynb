{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kr-vivekk/quantumML/blob/main/QCNN_USING_PYTORCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEuyJzqIQHwH",
        "outputId": "860ca651-b85d-45d3-9d83-65166294927e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.38.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\n",
            "Collecting pennylane-lightning>=0.38 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.8.30)\n",
            "Downloading PennyLane-0.38.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.6.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: appdirs, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.6.12 pennylane-0.38.0 pennylane-lightning-0.38.0 rustworkx-0.15.1\n",
            "Collecting quimb\n",
            "  Downloading quimb-1.8.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: autoray>=0.6.12 in /usr/local/lib/python3.10/dist-packages (from quimb) (0.6.12)\n",
            "Collecting cotengra>=0.6.1 (from quimb)\n",
            "  Downloading cotengra-0.6.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting cytoolz>=0.8.0 (from quimb)\n",
            "  Downloading cytoolz-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.10/dist-packages (from quimb) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from quimb) (1.26.4)\n",
            "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from quimb) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from quimb) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.10/dist-packages (from quimb) (4.66.5)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.39->quimb) (0.43.0)\n",
            "Downloading quimb-1.8.4-py3-none-any.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cotengra-0.6.2-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cytoolz, cotengra, quimb\n",
            "Successfully installed cotengra-0.6.2 cytoolz-1.0.0 quimb-1.8.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane\n",
        "!pip install quimb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBsl17Y2QVaw",
        "outputId": "881a9735-57c9-4ce0-8606-92049258249d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU2JcP3iBTfW"
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ia248QADQ5y",
        "outputId": "1bc2af20-c2ad-48e9-f828-cc7e6176280a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy6yaE4U4SO8"
      },
      "outputs": [],
      "source": [
        "image_size = (64, 64)  # Resize images to 64x64\n",
        "n_classes = 3  # 3 classes: COVID, Normal, Viral\n",
        "n_qubits = 4   # Number of qubits for quantum classifier\n",
        "n_layers = 4   # Number of layers in the quantum circuit\n",
        "epochs = 10    # Training epochs\n",
        "batch_size = 16 # Batch size for training\n",
        "images_per_class = 160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04yyhLN94S_3"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"/content/drive/MyDrive/Finalproject/dataset\"\n",
        "classes = ['COVID', 'Normal', 'Viral']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JDiDZzRb4VTR"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to hold the data and labels\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Loop through each class subfolder\n",
        "for class_name in classes:\n",
        "    class_path = dataset_dir + '/' + class_name\n",
        "\n",
        "    # Ensure the class folder exists\n",
        "    if not os.path.isdir(class_path):\n",
        "        print(f\"Directory {class_path} does not exist.\")\n",
        "        continue\n",
        "\n",
        "    # List all image files in the class subfolder\n",
        "    image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
        "\n",
        "    # Shuffle the files to ensure randomness\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    # Select 200 images only (or less if fewer images are available)\n",
        "    selected_images = image_files[:images_per_class]\n",
        "\n",
        "    # Process each image\n",
        "    for image_file in selected_images:\n",
        "        # Load the image\n",
        "        image_path = os.path.join(class_path, image_file)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Resize image (optional, based on your model requirements)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, image_size)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            data.append(image)\n",
        "            labels.append(class_name)\n",
        "        else:\n",
        "            print(f\"Error loading image: {image_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EVGgEAWP4Xc2"
      },
      "outputs": [],
      "source": [
        "data = np.array(data)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "grCvvZqQ4ZhC"
      },
      "outputs": [],
      "source": [
        "data = data/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V4qHy7X6Grsr",
        "outputId": "f414a547-4d1f-4321-ad61-b16acd54268f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(480, 64, 64)\n",
            "(480,)\n"
          ]
        }
      ],
      "source": [
        "print(data.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Orm2bUbnIgMt",
        "outputId": "e6148f9b-3343-46c0-e411-3476629267b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pennylane.numpy.tensor.tensor'>\n",
            "<class 'pennylane.numpy.tensor.tensor'>\n"
          ]
        }
      ],
      "source": [
        "print(type(data))\n",
        "print(type(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "afPk8JpCKkoG"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lguTh9Y0KJ0S"
      },
      "outputs": [],
      "source": [
        "data_torch = torch.tensor(data, dtype=torch.float32)\n",
        "labels_torch = torch.tensor(labels, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lMOIrqslK7F3"
      },
      "outputs": [],
      "source": [
        "data_torch = data_torch.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "josWLi1yLEml",
        "outputId": "bce016f1-59f8-46a8-c797-0edae4f8c9a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([480, 1, 64, 64])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_torch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_IaBW3VwLGxH"
      },
      "outputs": [],
      "source": [
        "# labels_one_hot = torch.nn.functional.one_hot(labels_torch, num_classes=n_classes).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5k7Ubg4vLjY-"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data_torch = data_torch.to(device)\n",
        "labels_torch = labels_torch.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uCEolcYLpBr",
        "outputId": "edb734d3-b2a5-44b4-e362-d6f98de794a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([480, 1, 64, 64])\n",
            "torch.Size([480])\n"
          ]
        }
      ],
      "source": [
        "print(data_torch.shape)\n",
        "print(labels_torch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YG-mPIzXLugG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create a dataset and dataloader\n",
        "dataset = TensorDataset(data_torch, labels_torch)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-PKC3w1uL5UV"
      },
      "outputs": [],
      "source": [
        "for images, labels in dataloader:\n",
        "    # Move images and labels to the device if not already\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RSPkAFx5GwFb"
      },
      "outputs": [],
      "source": [
        "n_qubits = 4\n",
        "dev = qml.device(\"default.mixed\", wires=n_qubits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "U8L9Mh3UG0Zc"
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\n",
        "def qnode(inputs, weights):\n",
        "    for depth in range(4):\n",
        "        for i in range(n_qubits):\n",
        "            qml.Hadamard(wires=i)\n",
        "            qml.RY(inputs[i], wires=i)\n",
        "            qml.RZ(inputs[i], wires=i)\n",
        "            qml.RX(inputs[i], wires=i)\n",
        "            qml.RY(inputs[i], wires=i)\n",
        "\n",
        "        qml.CNOT(wires=[0, 1])\n",
        "        qml.CNOT(wires=[2, 3])\n",
        "        qml.CNOT(wires=[1, 2])\n",
        "\n",
        "        for i in range(n_qubits):\n",
        "            qml.RY(weights[0][i], wires=i)\n",
        "            qml.RZ(weights[1][i], wires=i)\n",
        "            qml.RX(weights[2][i], wires=i)\n",
        "            qml.RY(weights[3][i], wires=i)\n",
        "\n",
        "\n",
        "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(3)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JmCr2qOAG27p"
      },
      "outputs": [],
      "source": [
        "n_layers = 4\n",
        "weight_shapes = {\"weights\": (n_layers, n_qubits)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RvurPtz2G54a"
      },
      "outputs": [],
      "source": [
        "qlayer = qml.qnn.TorchLayer(qnode, weight_shapes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "P37CT4FSHBfJ"
      },
      "outputs": [],
      "source": [
        "class HybridModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=2, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 1024)  # Adjust based on input size\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 16)\n",
        "        self.qlayer = qlayer\n",
        "        self.output = nn.Linear(3, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.qlayer(x)\n",
        "        x = torch.reshape(x, (x.size(0), -1))\n",
        "        x = self.output(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "O_ytXDL4H2g7"
      },
      "outputs": [],
      "source": [
        "model = HybridModel().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngtmuaa1reeu",
        "outputId": "58ba4e88-4ce7-4b25-be5d-bd98a2d62e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRqeaVZpMEZV",
        "outputId": "a76342c6-8cea-4d5e-9f16-e0704b92409e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HybridModel(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(32, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=16, bias=True)\n",
            "  (qlayer): <Quantum Torch Layer: func=qnode>\n",
            "  (output): Linear(in_features=3, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bI8vXR0NrlsI"
      },
      "outputs": [],
      "source": [
        "# model.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PhAQVJAhuvZg"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1hWNL8LMGNu",
        "outputId": "5a7a06a1-5aed-40a0-c221-17e88f4f1d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.1178, Accuracy: 33.54%\n",
            "Epoch [2/10], Loss: 1.1142, Accuracy: 34.38%\n",
            "Epoch [3/10], Loss: 1.1121, Accuracy: 32.71%\n",
            "Epoch [4/10], Loss: 1.1158, Accuracy: 33.96%\n",
            "Epoch [5/10], Loss: 1.1140, Accuracy: 32.92%\n",
            "Epoch [6/10], Loss: 1.1134, Accuracy: 33.33%\n",
            "Epoch [7/10], Loss: 1.1129, Accuracy: 33.96%\n",
            "Epoch [8/10], Loss: 1.1072, Accuracy: 32.50%\n",
            "Epoch [9/10], Loss: 1.1062, Accuracy: 34.38%\n",
            "Epoch [10/10], Loss: 1.1182, Accuracy: 30.63%\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.0001)  # Adjusted learning rate\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_accuracy = correct / total * 100\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BwTxj0qyhp1",
        "outputId": "f8952bc8-1564-4354-f352-81a9434b29f0",
        "collapsed": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('conv1.weight',\n",
              "              tensor([[[[-0.2480,  0.3517],\n",
              "                        [-0.2035, -0.2497]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.4381, -0.3219],\n",
              "                        [-0.3220,  0.2392]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1475, -0.3571],\n",
              "                        [ 0.2318, -0.0213]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.1538, -0.3320],\n",
              "                        [-0.0327, -0.1388]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.4098, -0.1546],\n",
              "                        [-0.4349, -0.3135]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1036,  0.0581],\n",
              "                        [-0.0572,  0.2856]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.4764,  0.0927],\n",
              "                        [-0.3887,  0.2466]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.3917,  0.2895],\n",
              "                        [ 0.1155, -0.2325]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.4374, -0.4009],\n",
              "                        [-0.3849, -0.3427]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0501, -0.1399],\n",
              "                        [ 0.0097, -0.4409]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2833, -0.3024],\n",
              "                        [ 0.1879, -0.4000]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0862, -0.0641],\n",
              "                        [-0.3819, -0.4221]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0155, -0.4937],\n",
              "                        [ 0.2231, -0.1848]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0805, -0.3213],\n",
              "                        [ 0.2164,  0.4451]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0071,  0.0156],\n",
              "                        [-0.1163, -0.2222]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.3226, -0.0259],\n",
              "                        [-0.4570, -0.2313]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.4303, -0.3525],\n",
              "                        [-0.4094,  0.2779]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3379,  0.2467],\n",
              "                        [-0.2593, -0.2962]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3070, -0.3292],\n",
              "                        [-0.0040, -0.2537]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.1838,  0.1208],\n",
              "                        [ 0.1401, -0.0204]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2570,  0.3761],\n",
              "                        [ 0.4172,  0.0922]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0330, -0.0883],\n",
              "                        [-0.5051,  0.4195]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3702,  0.4176],\n",
              "                        [ 0.3961, -0.2674]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2872,  0.3359],\n",
              "                        [-0.1743, -0.1419]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1858,  0.1310],\n",
              "                        [-0.4088, -0.0457]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2648,  0.3192],\n",
              "                        [-0.1269, -0.3088]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1289,  0.2099],\n",
              "                        [ 0.2099, -0.4630]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0267,  0.2855],\n",
              "                        [-0.2566, -0.0921]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1043,  0.3659],\n",
              "                        [ 0.4473, -0.3153]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.2170, -0.0799],\n",
              "                        [-0.3664,  0.1057]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.1319, -0.0451],\n",
              "                        [ 0.2540, -0.2009]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3305,  0.4530],\n",
              "                        [-0.2525, -0.1417]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3548, -0.3479],\n",
              "                        [-0.1172,  0.0426]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2116,  0.0624],\n",
              "                        [-0.0835,  0.0543]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0055, -0.4151],\n",
              "                        [ 0.1491, -0.0255]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0471,  0.4785],\n",
              "                        [ 0.2692, -0.4341]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0131,  0.0741],\n",
              "                        [-0.0116, -0.2488]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0171,  0.0460],\n",
              "                        [-0.0403,  0.3814]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0522, -0.2170],\n",
              "                        [-0.2988, -0.1000]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2494, -0.2261],\n",
              "                        [-0.3345, -0.1738]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.4583, -0.4539],\n",
              "                        [ 0.0975, -0.0459]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.2907, -0.0921],\n",
              "                        [-0.2363,  0.0944]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.4875,  0.1902],\n",
              "                        [ 0.4669,  0.2201]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0018, -0.0514],\n",
              "                        [-0.0956,  0.3845]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1539,  0.0787],\n",
              "                        [ 0.4827,  0.1618]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.3003,  0.3123],\n",
              "                        [ 0.0545, -0.3683]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.4327,  0.1064],\n",
              "                        [-0.1570, -0.0718]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.3689,  0.3974],\n",
              "                        [ 0.2338, -0.4309]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.3472,  0.4337],\n",
              "                        [-0.2887,  0.0959]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0731,  0.1232],\n",
              "                        [ 0.0709, -0.2620]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2244,  0.1027],\n",
              "                        [ 0.0324,  0.0812]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3087, -0.0050],\n",
              "                        [-0.3360,  0.4700]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.4347,  0.2066],\n",
              "                        [ 0.1182,  0.3452]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1063,  0.2076],\n",
              "                        [-0.2990,  0.2625]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.2735,  0.3201],\n",
              "                        [-0.2363,  0.4336]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1117,  0.3947],\n",
              "                        [ 0.4747, -0.3238]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2663,  0.2149],\n",
              "                        [ 0.1514,  0.1821]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0934, -0.1536],\n",
              "                        [-0.3321,  0.0008]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0866, -0.1800],\n",
              "                        [ 0.3206, -0.1420]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0695,  0.0564],\n",
              "                        [-0.3933, -0.0300]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0259,  0.1232],\n",
              "                        [ 0.1046,  0.3071]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.0331,  0.4186],\n",
              "                        [-0.0441, -0.3221]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.2603, -0.3845],\n",
              "                        [ 0.4636, -0.0377]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.2096,  0.0338],\n",
              "                        [ 0.1391, -0.0976]]]], device='cuda:0')),\n",
              "             ('conv1.bias',\n",
              "              tensor([ 0.0514,  0.1093, -0.4469,  0.4558,  0.1671, -0.3220,  0.0731,  0.3496,\n",
              "                       0.1218, -0.0115,  0.3191, -0.0766, -0.4751, -0.4442, -0.1092,  0.4586,\n",
              "                      -0.4131, -0.0850, -0.4398, -0.2998, -0.3541, -0.0348,  0.3864, -0.2715,\n",
              "                      -0.0975, -0.3057, -0.0371, -0.4622,  0.3417, -0.4231, -0.2501, -0.4109,\n",
              "                       0.1810,  0.3743,  0.3081, -0.3542, -0.1504, -0.4324,  0.4891,  0.4373,\n",
              "                      -0.3673,  0.4106, -0.4171, -0.3580, -0.4803,  0.3338, -0.0042, -0.1836,\n",
              "                      -0.2353,  0.0193,  0.3231,  0.1750,  0.2848, -0.4160, -0.3064, -0.0832,\n",
              "                      -0.1678, -0.1447,  0.1401,  0.4058,  0.1106,  0.4689, -0.1507,  0.1861],\n",
              "                     device='cuda:0')),\n",
              "             ('conv2.weight',\n",
              "              tensor([[[[-1.7912e-03, -5.7345e-02],\n",
              "                        [ 2.9587e-02,  2.9819e-02]],\n",
              "              \n",
              "                       [[ 4.4329e-02,  9.9990e-04],\n",
              "                        [ 1.0978e-02,  1.0018e-02]],\n",
              "              \n",
              "                       [[-2.4717e-02,  4.2888e-02],\n",
              "                        [ 5.7315e-02, -6.0095e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 3.6756e-02,  3.2579e-02],\n",
              "                        [ 3.2882e-02,  1.4952e-02]],\n",
              "              \n",
              "                       [[ 1.0195e-02, -5.7103e-02],\n",
              "                        [-5.3165e-02,  2.6103e-02]],\n",
              "              \n",
              "                       [[-7.0912e-03, -2.7258e-02],\n",
              "                        [-2.3856e-02, -6.2630e-03]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 1.2737e-02, -5.5812e-03],\n",
              "                        [-2.8010e-02, -6.7355e-02]],\n",
              "              \n",
              "                       [[ 2.8162e-02,  3.2779e-02],\n",
              "                        [-5.2425e-02, -3.3505e-02]],\n",
              "              \n",
              "                       [[-3.5229e-02, -1.9976e-03],\n",
              "                        [-4.4088e-02,  5.3109e-03]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 4.1605e-02, -7.0952e-03],\n",
              "                        [ 1.9437e-02,  4.8105e-02]],\n",
              "              \n",
              "                       [[-4.9391e-02,  6.4202e-02],\n",
              "                        [ 6.0139e-03,  4.3966e-03]],\n",
              "              \n",
              "                       [[-5.8061e-02, -2.0547e-02],\n",
              "                        [ 4.6392e-02,  1.2587e-02]]],\n",
              "              \n",
              "              \n",
              "                      [[[-4.3221e-02, -5.3225e-03],\n",
              "                        [-2.9385e-02, -4.2627e-02]],\n",
              "              \n",
              "                       [[-4.5084e-02, -5.4662e-02],\n",
              "                        [ 4.0906e-03,  2.5055e-02]],\n",
              "              \n",
              "                       [[ 3.2605e-02, -5.4366e-02],\n",
              "                        [-4.3178e-02,  3.2043e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-6.9725e-02,  4.1908e-02],\n",
              "                        [-8.5886e-03,  2.1000e-02]],\n",
              "              \n",
              "                       [[ 9.8872e-03,  3.2505e-02],\n",
              "                        [-2.0200e-02, -1.6389e-02]],\n",
              "              \n",
              "                       [[-4.9956e-02, -1.7729e-02],\n",
              "                        [-1.2951e-02, -1.2713e-02]]],\n",
              "              \n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "              \n",
              "                      [[[-2.3877e-02,  3.8721e-02],\n",
              "                        [ 5.7429e-02,  1.6902e-02]],\n",
              "              \n",
              "                       [[ 4.9363e-02,  6.6707e-02],\n",
              "                        [ 3.5789e-02,  7.9391e-02]],\n",
              "              \n",
              "                       [[ 3.4403e-02, -1.6202e-02],\n",
              "                        [ 1.8844e-02,  1.1994e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 1.3734e-02,  2.0459e-02],\n",
              "                        [-6.4740e-03,  5.5803e-02]],\n",
              "              \n",
              "                       [[ 1.4088e-02,  6.7914e-03],\n",
              "                        [-7.9746e-03,  8.2282e-02]],\n",
              "              \n",
              "                       [[ 6.4404e-02,  6.6835e-03],\n",
              "                        [ 5.3902e-02, -5.6595e-04]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 3.2303e-02, -2.5091e-02],\n",
              "                        [ 6.1604e-02,  6.4433e-03]],\n",
              "              \n",
              "                       [[ 8.7792e-03, -6.1455e-02],\n",
              "                        [ 2.5029e-02, -1.6379e-02]],\n",
              "              \n",
              "                       [[-3.2864e-02,  4.1950e-02],\n",
              "                        [ 1.6584e-03,  5.5358e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 1.0103e-02,  2.3354e-02],\n",
              "                        [ 3.4325e-02,  2.6719e-02]],\n",
              "              \n",
              "                       [[ 4.3119e-02, -5.1910e-02],\n",
              "                        [-4.6271e-02, -9.6317e-03]],\n",
              "              \n",
              "                       [[ 3.9905e-02,  1.5827e-02],\n",
              "                        [-4.6225e-02,  1.8255e-02]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 5.1142e-02,  1.7120e-02],\n",
              "                        [-6.7142e-03, -2.6689e-03]],\n",
              "              \n",
              "                       [[ 2.4488e-03, -5.9462e-03],\n",
              "                        [-5.8243e-03, -6.1233e-02]],\n",
              "              \n",
              "                       [[ 6.6433e-03, -1.2515e-02],\n",
              "                        [ 4.6613e-02, -3.6569e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-6.3265e-05, -3.7041e-02],\n",
              "                        [-2.9277e-02, -4.7320e-02]],\n",
              "              \n",
              "                       [[-4.1947e-03, -2.5284e-02],\n",
              "                        [-5.2811e-02,  5.8373e-03]],\n",
              "              \n",
              "                       [[-5.6044e-02,  8.4486e-03],\n",
              "                        [-2.8198e-02, -3.6487e-02]]]], device='cuda:0')),\n",
              "             ('conv2.bias',\n",
              "              tensor([-0.0530,  0.0240,  0.0326,  0.0089,  0.0077, -0.0310,  0.0142,  0.0348,\n",
              "                       0.0374, -0.0724,  0.0088, -0.0226, -0.0421,  0.0130,  0.0180,  0.0497,\n",
              "                       0.0307,  0.0339, -0.0050, -0.0522, -0.0686, -0.0475, -0.0146, -0.0245,\n",
              "                       0.0178,  0.0202,  0.0442, -0.0377, -0.0385,  0.0522, -0.0438, -0.0699],\n",
              "                     device='cuda:0')),\n",
              "             ('conv3.weight',\n",
              "              tensor([[[[-5.4717e-02, -3.9563e-02],\n",
              "                        [ 2.8230e-02,  1.9009e-03]],\n",
              "              \n",
              "                       [[-5.0324e-02, -6.1921e-02],\n",
              "                        [-1.8439e-02, -3.9051e-02]],\n",
              "              \n",
              "                       [[ 8.0320e-02, -8.3939e-02],\n",
              "                        [-5.1731e-02,  7.8104e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-2.1452e-02, -2.1971e-02],\n",
              "                        [ 8.5314e-05, -1.0120e-01]],\n",
              "              \n",
              "                       [[ 2.1467e-02, -3.1005e-02],\n",
              "                        [-3.6323e-02, -5.0348e-02]],\n",
              "              \n",
              "                       [[ 6.2685e-02,  3.0667e-02],\n",
              "                        [ 5.0753e-02, -1.2328e-04]]],\n",
              "              \n",
              "              \n",
              "                      [[[-2.4374e-02,  3.6054e-02],\n",
              "                        [-1.2190e-02,  4.8837e-02]],\n",
              "              \n",
              "                       [[-6.8750e-02, -9.7171e-03],\n",
              "                        [ 7.1312e-02, -7.7978e-02]],\n",
              "              \n",
              "                       [[-8.7876e-02,  4.4296e-02],\n",
              "                        [-6.5823e-02,  5.3520e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-1.0693e-01, -3.2608e-02],\n",
              "                        [ 4.3361e-02, -7.4928e-02]],\n",
              "              \n",
              "                       [[ 1.6993e-02,  1.5045e-02],\n",
              "                        [-6.1787e-03, -1.2096e-02]],\n",
              "              \n",
              "                       [[ 2.4258e-02, -9.2116e-02],\n",
              "                        [-6.5605e-03, -8.6579e-03]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 3.1474e-02,  3.3384e-02],\n",
              "                        [-1.2097e-02,  6.8104e-02]],\n",
              "              \n",
              "                       [[-5.1823e-02,  3.4053e-02],\n",
              "                        [ 1.2646e-03,  5.7169e-02]],\n",
              "              \n",
              "                       [[-4.2753e-02,  3.4103e-02],\n",
              "                        [-6.6721e-02,  5.8074e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-1.0159e-01, -1.0009e-01],\n",
              "                        [ 5.2799e-02, -1.9366e-02]],\n",
              "              \n",
              "                       [[-3.5780e-02, -6.9705e-02],\n",
              "                        [-4.0778e-02,  1.1011e-02]],\n",
              "              \n",
              "                       [[ 2.4338e-02,  6.8627e-02],\n",
              "                        [ 1.2614e-04,  3.4731e-02]]],\n",
              "              \n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "              \n",
              "                      [[[ 3.1052e-02, -2.8099e-02],\n",
              "                        [-5.2158e-02,  7.6773e-02]],\n",
              "              \n",
              "                       [[-4.3566e-02,  7.8336e-03],\n",
              "                        [-7.4815e-02, -2.1259e-02]],\n",
              "              \n",
              "                       [[ 4.8626e-02,  7.1815e-02],\n",
              "                        [ 7.5119e-02, -1.6749e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 4.8136e-02, -2.5747e-02],\n",
              "                        [ 3.6775e-02,  1.8148e-03]],\n",
              "              \n",
              "                       [[ 1.6127e-02,  7.7125e-02],\n",
              "                        [-5.1018e-02, -2.1764e-02]],\n",
              "              \n",
              "                       [[ 4.6132e-02, -4.7755e-02],\n",
              "                        [ 1.2797e-02, -4.1091e-03]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 4.2047e-02,  3.0302e-02],\n",
              "                        [ 5.9534e-02, -7.0528e-02]],\n",
              "              \n",
              "                       [[-3.5105e-02, -1.7875e-02],\n",
              "                        [-4.6565e-02,  4.9315e-02]],\n",
              "              \n",
              "                       [[-3.5181e-02, -6.6789e-02],\n",
              "                        [ 7.9984e-03, -7.7411e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 2.1740e-02, -2.7521e-03],\n",
              "                        [-7.7152e-02, -6.9218e-02]],\n",
              "              \n",
              "                       [[ 1.8974e-02, -8.7803e-02],\n",
              "                        [ 4.4761e-02, -5.6024e-02]],\n",
              "              \n",
              "                       [[-1.0371e-02,  4.8100e-03],\n",
              "                        [-5.5341e-02,  4.2098e-02]]],\n",
              "              \n",
              "              \n",
              "                      [[[-6.3730e-03,  4.4179e-02],\n",
              "                        [ 2.5132e-02, -1.6517e-03]],\n",
              "              \n",
              "                       [[-2.7683e-02, -3.4888e-02],\n",
              "                        [ 5.8887e-02,  2.3466e-02]],\n",
              "              \n",
              "                       [[-8.3184e-02, -7.7964e-02],\n",
              "                        [ 1.8026e-03,  2.4409e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-1.0386e-01, -9.9179e-02],\n",
              "                        [-6.3658e-02, -2.5864e-03]],\n",
              "              \n",
              "                       [[-8.9371e-03,  1.6793e-02],\n",
              "                        [-3.1792e-04,  6.9825e-02]],\n",
              "              \n",
              "                       [[-5.5829e-04, -4.8475e-02],\n",
              "                        [ 7.0680e-02, -4.5639e-02]]]], device='cuda:0')),\n",
              "             ('conv3.bias',\n",
              "              tensor([-0.0263, -0.0472,  0.0377,  0.0267,  0.0545, -0.0486,  0.0014,  0.0123,\n",
              "                      -0.0853,  0.0513, -0.0593, -0.0692,  0.0313,  0.0088, -0.0913,  0.0097,\n",
              "                      -0.0788,  0.0415, -0.0372,  0.0539, -0.0095,  0.0107, -0.0285, -0.0915,\n",
              "                      -0.0071,  0.0673,  0.0112, -0.0711,  0.0624, -0.0755,  0.0441,  0.0021],\n",
              "                     device='cuda:0')),\n",
              "             ('fc1.weight',\n",
              "              tensor([[-0.0030,  0.0027, -0.0015,  ..., -0.0191,  0.0169,  0.0014],\n",
              "                      [ 0.0176, -0.0026,  0.0175,  ..., -0.0232, -0.0079, -0.0110],\n",
              "                      [ 0.0172, -0.0093,  0.0170,  ...,  0.0058,  0.0179, -0.0178],\n",
              "                      ...,\n",
              "                      [-0.0152, -0.0017, -0.0011,  ...,  0.0106, -0.0288, -0.0271],\n",
              "                      [-0.0139, -0.0052,  0.0003,  ..., -0.0005, -0.0166, -0.0060],\n",
              "                      [ 0.0113, -0.0100, -0.0075,  ..., -0.0162, -0.0134,  0.0220]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc1.bias',\n",
              "              tensor([ 0.0086, -0.0159, -0.0162,  ..., -0.0143, -0.0246,  0.0026],\n",
              "                     device='cuda:0')),\n",
              "             ('fc2.weight',\n",
              "              tensor([[-0.0100, -0.0108, -0.0208,  ..., -0.0242, -0.0191,  0.0115],\n",
              "                      [-0.0301,  0.0214,  0.0205,  ..., -0.0106,  0.0052, -0.0143],\n",
              "                      [ 0.0108,  0.0084,  0.0254,  ...,  0.0181,  0.0005, -0.0206],\n",
              "                      ...,\n",
              "                      [ 0.0134, -0.0184, -0.0216,  ..., -0.0090,  0.0255, -0.0307],\n",
              "                      [ 0.0129, -0.0248,  0.0244,  ...,  0.0011, -0.0015,  0.0022],\n",
              "                      [ 0.0190, -0.0143,  0.0168,  ..., -0.0171,  0.0042, -0.0071]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc2.bias',\n",
              "              tensor([ 1.9127e-02, -2.1757e-03, -2.3596e-02, -3.0335e-02,  2.0720e-03,\n",
              "                       4.9997e-02, -2.8596e-02, -2.8908e-02,  2.6032e-02,  9.6948e-03,\n",
              "                       4.9330e-05,  1.8472e-02,  9.0190e-02,  2.0090e-02,  3.3179e-03,\n",
              "                      -1.3329e-02,  1.4076e-03, -2.2877e-02, -1.5782e-02, -9.4672e-03,\n",
              "                      -2.5539e-02, -2.4784e-02, -5.0890e-02, -5.4707e-04, -1.5951e-02,\n",
              "                       5.7437e-03,  1.7582e-02, -2.9725e-02, -2.6815e-02, -4.3589e-02,\n",
              "                       7.8771e-03, -2.9691e-02, -5.9952e-03,  1.3363e-02, -4.1569e-02,\n",
              "                       2.4907e-02, -3.6628e-02,  1.9603e-02,  8.1540e-03, -2.3063e-02,\n",
              "                      -3.1070e-02, -3.3305e-02,  1.2418e-03,  6.2067e-02,  1.9168e-03,\n",
              "                      -2.1259e-02,  2.9411e-04,  7.7216e-03, -1.5919e-02, -1.3596e-02,\n",
              "                      -1.0330e-02, -2.3198e-03,  1.2203e-02,  1.0783e-02, -3.1821e-02,\n",
              "                       3.1011e-02,  8.5255e-04, -3.7174e-02,  3.4555e-03,  1.0512e-02,\n",
              "                       5.8749e-03, -2.9718e-02,  6.5376e-03, -1.0364e-02, -3.3768e-02,\n",
              "                       2.4419e-02,  3.0373e-02,  2.1086e-02, -1.1513e-02,  2.1203e-02,\n",
              "                      -3.0757e-02,  2.3904e-02,  2.2496e-02, -3.3443e-03, -2.7499e-02,\n",
              "                       3.3212e-03,  1.5134e-02, -3.1942e-02, -2.5389e-02, -1.1291e-03,\n",
              "                       1.8659e-02,  2.5266e-04,  1.2736e-02,  5.3453e-02,  3.1190e-02,\n",
              "                       6.3675e-02,  8.8656e-04,  5.8815e-03, -3.5418e-02, -1.4386e-03,\n",
              "                       4.1568e-03, -2.5679e-02,  2.4682e-02,  3.3687e-02,  1.0931e-02,\n",
              "                       1.7848e-02,  2.9106e-02,  2.2721e-02,  2.2033e-02, -3.0821e-02,\n",
              "                       2.4381e-02, -2.6575e-02, -2.4822e-03, -2.9454e-02, -7.3445e-03,\n",
              "                       1.4715e-02, -8.1064e-03, -1.4890e-02, -2.1700e-02,  3.2913e-03,\n",
              "                       5.7037e-02, -9.1953e-03,  2.3200e-03, -1.8053e-04, -9.5529e-03,\n",
              "                      -1.9286e-02, -1.6455e-02, -4.3029e-03, -2.0045e-03,  1.2747e-03,\n",
              "                      -1.0665e-02,  7.8375e-03, -2.3475e-03, -2.8624e-02, -2.0259e-02,\n",
              "                      -3.4721e-02, -2.2086e-02, -2.1766e-02, -2.0557e-02, -2.3838e-02,\n",
              "                       7.9399e-03,  1.2140e-02, -1.6062e-02,  8.8457e-03, -2.3839e-02,\n",
              "                      -1.7176e-03,  6.1757e-03, -3.8674e-02, -3.4790e-03,  2.7190e-02,\n",
              "                       3.3816e-03,  2.5161e-03,  1.6169e-03,  1.1390e-02, -2.4533e-03,\n",
              "                      -2.3949e-02, -6.3372e-03, -1.3715e-02, -2.8992e-02, -4.5410e-03,\n",
              "                      -3.0212e-02,  1.0378e-02,  1.1408e-02, -3.4345e-02, -3.1401e-02,\n",
              "                      -9.1783e-03, -1.8437e-02, -2.6705e-02, -5.4040e-03, -2.6392e-02,\n",
              "                      -2.3326e-03, -1.8462e-03, -2.6478e-02, -1.4478e-02, -3.1096e-02,\n",
              "                      -2.2248e-02, -5.0662e-03, -3.8723e-03, -1.1050e-02,  1.6983e-02,\n",
              "                      -2.2448e-02, -1.1731e-02,  5.0101e-03,  1.5980e-02, -1.2212e-04,\n",
              "                      -2.5477e-02,  2.6727e-02,  1.4633e-02,  2.3452e-03,  3.4447e-02,\n",
              "                      -2.9628e-02, -9.5329e-03,  4.3812e-02, -7.4926e-03, -2.6441e-03,\n",
              "                      -3.2133e-03, -3.7412e-02,  1.4308e-02, -2.7053e-02,  6.1614e-03,\n",
              "                      -2.7473e-02,  2.0362e-02, -2.4374e-02,  9.5162e-03,  3.3241e-02,\n",
              "                       2.5511e-02,  1.9847e-02,  2.0882e-02, -4.2338e-02, -4.0494e-03,\n",
              "                      -8.5907e-03,  9.7969e-03,  5.8441e-03, -6.8086e-03,  1.1935e-02,\n",
              "                      -8.2025e-03, -3.8212e-02,  9.2990e-03, -3.8747e-02,  7.4447e-03,\n",
              "                       4.9642e-02,  3.5681e-02,  2.5278e-02, -2.7750e-02, -9.0522e-03,\n",
              "                      -2.4669e-02, -2.8395e-02, -1.6216e-02,  3.7125e-02, -1.4469e-02,\n",
              "                      -3.0584e-02,  3.4490e-03, -2.6066e-02,  3.6286e-02,  2.3891e-02,\n",
              "                       6.2826e-02, -2.7611e-03, -2.8406e-02, -6.9237e-03, -6.2631e-03,\n",
              "                      -3.1031e-02,  7.4173e-03, -1.2091e-02, -1.5905e-02,  3.8426e-02,\n",
              "                      -1.6251e-02, -1.9752e-03, -2.2494e-02,  2.2386e-02, -2.1378e-02,\n",
              "                       1.6001e-02, -3.1064e-02, -8.1110e-03, -1.7363e-02, -1.2647e-02,\n",
              "                      -1.2609e-02,  9.5889e-03, -3.3331e-03, -2.8798e-02,  2.8640e-02,\n",
              "                      -1.7169e-02, -1.0873e-02, -1.0550e-03,  3.3664e-02,  1.1756e-03,\n",
              "                       1.3065e-02, -6.7271e-04, -3.7028e-02,  1.4435e-02, -3.9126e-02,\n",
              "                       4.1391e-02,  1.6150e-02,  6.7857e-03, -1.5342e-02, -2.3590e-02,\n",
              "                       4.3509e-02,  3.5679e-02, -2.2425e-02,  9.7515e-04, -3.3208e-02,\n",
              "                       1.1440e-02,  1.6409e-02,  1.6051e-02,  2.3814e-02,  3.2136e-03,\n",
              "                       2.2929e-02, -7.0699e-03,  3.8645e-02, -2.8945e-02, -1.1083e-02,\n",
              "                       1.7445e-02, -3.6569e-02, -2.0292e-02, -4.1575e-03, -1.5025e-02,\n",
              "                       1.3616e-02,  1.8892e-02, -6.4287e-03, -1.3774e-02,  2.3263e-02,\n",
              "                       1.5570e-02, -3.9031e-02,  2.1906e-02, -2.1226e-02, -3.3986e-02,\n",
              "                       7.5391e-04, -3.4122e-02, -7.7645e-03,  6.3675e-03,  1.3465e-02,\n",
              "                       3.1558e-02, -3.8957e-02, -3.8337e-02, -1.4580e-02, -3.2864e-03,\n",
              "                      -7.6702e-03, -1.7861e-02, -1.0961e-02, -1.2066e-02, -3.0416e-02,\n",
              "                       8.6706e-06, -3.2319e-03,  5.3537e-03,  7.5287e-03,  4.5910e-02,\n",
              "                       7.7551e-03, -2.3581e-02,  2.6995e-02,  3.8832e-03, -3.7019e-03,\n",
              "                      -1.6365e-02, -1.9942e-02, -4.2724e-02,  1.8108e-02,  8.8480e-03,\n",
              "                      -3.5408e-02, -3.4649e-02,  1.8126e-02, -9.4748e-03, -1.0327e-02,\n",
              "                       1.7844e-02,  7.4014e-03,  1.1721e-03,  2.0985e-02, -1.6360e-02,\n",
              "                      -3.2548e-02,  1.2420e-02, -1.0834e-02,  1.7433e-02,  3.2542e-02,\n",
              "                      -2.2920e-02, -3.5184e-02, -2.5439e-02,  5.2747e-02, -1.2854e-02,\n",
              "                      -3.4780e-03,  2.0552e-02,  3.9634e-02,  8.0069e-03, -8.2369e-03,\n",
              "                       1.6362e-02,  1.8472e-02, -1.0138e-02, -8.3070e-03, -1.4880e-02,\n",
              "                      -1.9325e-02,  5.2735e-03, -4.3011e-02, -1.7695e-02,  2.4813e-03,\n",
              "                       8.4898e-03, -1.3189e-02, -3.5802e-02, -3.2804e-02, -8.7285e-03,\n",
              "                      -2.7810e-02, -3.0778e-02,  5.7022e-04, -1.9968e-02, -2.5857e-02,\n",
              "                       2.5922e-02, -1.6128e-02,  1.6242e-02,  3.8899e-02,  1.6754e-02,\n",
              "                       9.4801e-03,  4.6755e-03,  1.4924e-02,  1.8234e-03,  3.6679e-03,\n",
              "                       8.4375e-04,  4.0936e-03,  2.3369e-02,  1.8464e-02,  7.9374e-03,\n",
              "                      -2.6012e-02,  4.2160e-02, -1.9395e-02,  3.4800e-02, -1.2571e-02,\n",
              "                      -2.9622e-02,  3.6501e-02, -9.5535e-03, -1.5255e-02, -3.3437e-02,\n",
              "                      -2.8143e-02, -1.5977e-02, -2.6189e-02, -2.1694e-02, -9.3867e-03,\n",
              "                      -1.1598e-02,  3.3042e-02, -1.3653e-02,  2.5310e-03, -1.2493e-02,\n",
              "                       1.8217e-02, -1.7604e-02, -6.0301e-03,  2.4572e-02,  1.0308e-02,\n",
              "                      -1.4423e-02,  2.5813e-02, -2.2052e-02,  1.9789e-02,  2.7249e-02,\n",
              "                       1.5314e-02,  1.5187e-02,  1.3560e-02, -2.1615e-02,  4.1673e-02,\n",
              "                      -2.5786e-02, -4.5982e-02, -1.3949e-02,  5.6389e-03,  4.7928e-02,\n",
              "                      -2.9322e-02, -4.0423e-02,  1.8528e-02, -1.2982e-03,  5.8039e-02,\n",
              "                      -1.9709e-02,  5.8680e-03, -1.4798e-02, -1.8819e-02, -3.7486e-02,\n",
              "                      -2.5230e-02, -1.4682e-03, -1.9529e-02, -1.2029e-02,  1.0666e-02,\n",
              "                      -1.5829e-02, -4.2776e-02,  1.2724e-02,  3.0937e-02,  9.3817e-03,\n",
              "                       6.0868e-02, -6.3235e-03, -4.5545e-03, -5.0334e-03,  1.7498e-02,\n",
              "                       1.2777e-02, -2.4125e-02, -6.6402e-03,  1.2850e-03,  2.7456e-02,\n",
              "                      -1.1517e-02, -5.0992e-02,  2.7945e-02,  8.8349e-04,  1.5933e-02,\n",
              "                      -1.8550e-02, -3.3913e-02,  7.5670e-03, -1.0104e-02,  9.3230e-05,\n",
              "                       1.7710e-02, -4.3261e-03, -3.9136e-02, -6.3223e-03, -4.0962e-03,\n",
              "                       4.3507e-03,  1.5332e-02, -1.0160e-02, -2.4472e-02, -2.6992e-03,\n",
              "                       2.0759e-02, -5.7194e-03, -1.3383e-02, -1.1793e-02, -1.4445e-02,\n",
              "                       2.3822e-02, -4.3439e-03,  2.3347e-03,  2.4610e-02, -4.6737e-03,\n",
              "                       4.5159e-02, -4.2126e-02, -1.3675e-03, -1.5206e-02, -9.5316e-03,\n",
              "                       1.7763e-02, -1.6002e-02, -3.5756e-02, -4.4153e-02,  6.5116e-03,\n",
              "                      -2.1243e-02, -4.0984e-02, -1.2715e-02, -1.4528e-02, -1.6845e-03,\n",
              "                      -3.3937e-02,  6.7710e-03,  1.5829e-02, -5.8239e-03,  7.7720e-03,\n",
              "                      -1.9021e-02,  6.6057e-02, -2.3854e-02, -7.7436e-03,  1.1286e-02,\n",
              "                      -1.5398e-02,  1.7696e-02], device='cuda:0')),\n",
              "             ('fc3.weight',\n",
              "              tensor([[ 0.0188, -0.0090,  0.0022,  ..., -0.0284,  0.0196, -0.0507],\n",
              "                      [-0.0259, -0.0338, -0.0263,  ...,  0.0345, -0.0554, -0.0751],\n",
              "                      [ 0.0112, -0.0010, -0.0055,  ...,  0.0409, -0.0104, -0.0390],\n",
              "                      ...,\n",
              "                      [ 0.0370,  0.0136, -0.0151,  ..., -0.0069, -0.0003,  0.0168],\n",
              "                      [-0.0286,  0.0141,  0.0355,  ..., -0.0011, -0.0422, -0.0424],\n",
              "                      [-0.0474,  0.0263, -0.0347,  ..., -0.0197, -0.0043, -0.0320]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc3.bias',\n",
              "              tensor([-0.0408,  0.0521,  0.0124, -0.0281, -0.0593, -0.0208, -0.0241,  0.0547,\n",
              "                      -0.0393, -0.0382,  0.0351,  0.0499, -0.0030,  0.0232,  0.0216, -0.0363,\n",
              "                      -0.0660,  0.0232, -0.0257,  0.0079,  0.0056,  0.0302,  0.0252, -0.0286,\n",
              "                      -0.0081, -0.0106, -0.0221, -0.0134,  0.0028, -0.0416,  0.0300, -0.0180,\n",
              "                      -0.0099, -0.0193,  0.0520,  0.0220,  0.0044,  0.0322, -0.0200, -0.0483,\n",
              "                      -0.0431, -0.0377, -0.0035,  0.0700, -0.0096,  0.0221, -0.0299, -0.0359,\n",
              "                      -0.0421,  0.0417,  0.0406, -0.0237,  0.0295,  0.0184,  0.0293,  0.0173,\n",
              "                       0.0162, -0.0235, -0.0341,  0.0300, -0.0443, -0.0160, -0.0023,  0.0111],\n",
              "                     device='cuda:0')),\n",
              "             ('fc4.weight',\n",
              "              tensor([[ 0.0373, -0.1228,  0.1008,  ...,  0.0623, -0.1159, -0.1021],\n",
              "                      [ 0.1028,  0.1022,  0.1050,  ..., -0.0095, -0.0828,  0.0533],\n",
              "                      [ 0.0526,  0.0054,  0.0127,  ...,  0.0741,  0.0149,  0.0978],\n",
              "                      ...,\n",
              "                      [-0.1215,  0.0567,  0.0067,  ...,  0.0622,  0.0506, -0.0342],\n",
              "                      [-0.0307,  0.0509,  0.0501,  ..., -0.0747, -0.0974,  0.0735],\n",
              "                      [ 0.0481, -0.0825,  0.0436,  ..., -0.1009,  0.0096, -0.0390]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc4.bias',\n",
              "              tensor([-0.0897,  0.0899,  0.0413,  0.0672,  0.0620, -0.0566, -0.0103, -0.0158,\n",
              "                      -0.1335, -0.1346, -0.0899,  0.1382,  0.0831, -0.0072,  0.0295,  0.0672],\n",
              "                     device='cuda:0')),\n",
              "             ('qlayer.weights',\n",
              "              tensor([[3.4926, 4.0937, 0.7120, 1.8767],\n",
              "                      [3.8998, 5.1204, 1.7775, 2.0730],\n",
              "                      [0.7416, 2.8951, 3.8321, 3.4303],\n",
              "                      [4.9319, 2.4057, 3.5652, 2.8978]], device='cuda:0')),\n",
              "             ('output.weight',\n",
              "              tensor([[-0.5042,  0.1705, -0.4105],\n",
              "                      [-0.2477, -0.3076, -0.2193],\n",
              "                      [ 0.5656, -0.4549,  0.4419]], device='cuda:0')),\n",
              "             ('output.bias',\n",
              "              tensor([ 0.4339, -0.0079, -0.4218], device='cuda:0'))])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=2, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 1024)  # Adjust based on input size\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 16)\n",
        "        self.output = nn.Linear(16, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, p=0.5)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.output(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "uvM4uCjMV9wK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = CNNModel().to(device)"
      ],
      "metadata": {
        "id": "od2ctzSBWgnF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "vjEt-isHqadt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1341aa28-ed47-44e4-8952-f99f2a629e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.2429, Accuracy: 90.62%\n",
            "Epoch [2/20], Loss: 0.2071, Accuracy: 91.46%\n",
            "Epoch [3/20], Loss: 0.1518, Accuracy: 95.83%\n",
            "Epoch [4/20], Loss: 0.1436, Accuracy: 95.21%\n",
            "Epoch [5/20], Loss: 0.1377, Accuracy: 96.04%\n",
            "Epoch [6/20], Loss: 0.1376, Accuracy: 94.79%\n",
            "Epoch [7/20], Loss: 0.1266, Accuracy: 94.17%\n",
            "Epoch [8/20], Loss: 0.0916, Accuracy: 98.12%\n",
            "Epoch [9/20], Loss: 0.0950, Accuracy: 97.08%\n",
            "Epoch [10/20], Loss: 0.0891, Accuracy: 97.29%\n",
            "Epoch [11/20], Loss: 0.0668, Accuracy: 98.12%\n",
            "Epoch [12/20], Loss: 0.0558, Accuracy: 98.12%\n",
            "Epoch [13/20], Loss: 0.0563, Accuracy: 98.12%\n",
            "Epoch [14/20], Loss: 0.0361, Accuracy: 99.17%\n",
            "Epoch [15/20], Loss: 0.0543, Accuracy: 97.71%\n",
            "Epoch [16/20], Loss: 0.0440, Accuracy: 98.54%\n",
            "Epoch [17/20], Loss: 0.0545, Accuracy: 98.12%\n",
            "Epoch [18/20], Loss: 0.0450, Accuracy: 98.96%\n",
            "Epoch [19/20], Loss: 0.0205, Accuracy: 100.00%\n",
            "Epoch [20/20], Loss: 0.0282, Accuracy: 99.38%\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model2.parameters(), lr=0.0001)  # Adjusted learning rate\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model2.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model2(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_accuracy = correct / total * 100\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aS0qs_3ZXTze"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOipTMSNbQ/6p1Z8e9TvMJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}